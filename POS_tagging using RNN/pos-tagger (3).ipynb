{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Objective:**","metadata":{}},{"cell_type":"markdown","source":"The process of classifying words into their __parts of speech__ and labeling them accordingly is known as **part-of-speech tagging** or **POS-tagging**. This project aims at assigning words their respective POS tags using different Recurrent neural network (RNN) models.","metadata":{}},{"cell_type":"markdown","source":"The analysis involves the following steps.\n1. Preprocessing data\n2. Using Word Embeddings\n3. Building Vanilla RNN model\n4. Building LSTM model\n5. Building GRU model\n6. Building Bidirectional LSTM model\n7. Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Load the following libraries.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom nltk.corpus import brown\nfrom nltk.corpus import treebank\nfrom nltk.corpus import conll2000\nimport seaborn as sns\nfrom gensim.models import KeyedVectors\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom nltk.data import find","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:47:02.862666Z","iopub.execute_input":"2021-10-14T04:47:02.863052Z","iopub.status.idle":"2021-10-14T04:47:10.576257Z","shell.execute_reply.started":"2021-10-14T04:47:02.862940Z","shell.execute_reply":"2021-10-14T04:47:10.575320Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### **1. Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Load POS tagged corpora from NLTK.\n\ntreebank_corpus = treebank.tagged_sents(tagset='universal')\n\nbrown_corpus = brown.tagged_sents(tagset='universal')\n\nconll_corpus = conll2000.tagged_sents(tagset='universal')\n\ntagged_sentences = treebank_corpus + brown_corpus + conll_corpus","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:47:16.767640Z","iopub.execute_input":"2021-10-14T04:47:16.768332Z","iopub.status.idle":"2021-10-14T04:47:16.817790Z","shell.execute_reply.started":"2021-10-14T04:47:16.768299Z","shell.execute_reply":"2021-10-14T04:47:16.816752Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Let's look at the data.\n\ntagged_sentences[11]","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:47:19.967885Z","iopub.execute_input":"2021-10-14T04:47:19.968551Z","iopub.status.idle":"2021-10-14T04:47:33.632819Z","shell.execute_reply.started":"2021-10-14T04:47:19.968493Z","shell.execute_reply":"2021-10-14T04:47:33.631903Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### **Divide data in words (X) and tags (Y)**","metadata":{}},{"cell_type":"markdown","source":"Since this is a **many-to-many** problem, each data point will be a different sentence of the corpora.\n\nEach data point will have multiple words in the **input sequence**. This is what we will refer to as **X**.\n\nEach word will have its correpsonding tag in the **output sequence**. This what we will refer to as **Y**.\n\nSample dataset:\n\n|                    X                        |                 Y                |\n|---------------------------------------------|----------------------------------|\n|   Mr. Vinken is chairman of Elsevier        |   NOUN NOUN VERB NOUN ADP NOUN   |\n|     We have no useful information           |      PRON VERB DET ADJ NOUN      |","metadata":{}},{"cell_type":"code","source":"X = [] # Store input sequence\nY = [] # Store output sequence\n\nfor sentence in tagged_sentences:\n    X_sentence = []\n    Y_sentence = []\n    for entity in sentence:         \n        X_sentence.append(entity[0])  # entity[0] contains the word\n        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n        \n    X.append(X_sentence)\n    Y.append(Y_sentence)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:47:39.997343Z","iopub.execute_input":"2021-10-14T04:47:39.997634Z","iopub.status.idle":"2021-10-14T04:47:49.701406Z","shell.execute_reply.started":"2021-10-14T04:47:39.997605Z","shell.execute_reply":"2021-10-14T04:47:49.700478Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"num_words = len(set([word.lower() for sentence in X for word in sentence]))\n\nnum_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n\nprint(\"Total number of tagged sentences: {}\".format(len(X)))\n\nprint(\"Vocabulary size: {}\".format(num_words))\n\nprint(\"Total number of tags: {}\".format(num_tags))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:47:51.334346Z","iopub.execute_input":"2021-10-14T04:47:51.334667Z","iopub.status.idle":"2021-10-14T04:47:52.021278Z","shell.execute_reply.started":"2021-10-14T04:47:51.334637Z","shell.execute_reply":"2021-10-14T04:47:52.020286Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Let's look at first data point.\n\nprint('sample X: ', X[0], '\\n')\n\nprint('sample Y: ', Y[0], '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:00.382460Z","iopub.execute_input":"2021-10-14T04:48:00.383040Z","iopub.status.idle":"2021-10-14T04:48:00.391659Z","shell.execute_reply.started":"2021-10-14T04:48:00.383006Z","shell.execute_reply":"2021-10-14T04:48:00.390658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Ensure that the length of input sequence equals the output sequence.\n\nprint(\"Length of first input sequence  : {}\".format(len(X[0])))\n\nprint(\"Length of first output sequence : {}\".format(len(Y[0])))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:06.178954Z","iopub.execute_input":"2021-10-14T04:48:06.179260Z","iopub.status.idle":"2021-10-14T04:48:06.185815Z","shell.execute_reply.started":"2021-10-14T04:48:06.179224Z","shell.execute_reply":"2021-10-14T04:48:06.184665Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### **Vectorise X and Y**","metadata":{}},{"cell_type":"markdown","source":"#### Encode X and Y to integer values\n\nWe'll use the Tokenizer() function from Keras library to encode text sequence to integer sequence","metadata":{}},{"cell_type":"code","source":"# Encode X\n\nword_tokenizer = Tokenizer() \n\nword_tokenizer.fit_on_texts(X)\n\nX_encoded = word_tokenizer.texts_to_sequences(X)  ","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:12.006841Z","iopub.execute_input":"2021-10-14T04:48:12.007430Z","iopub.status.idle":"2021-10-14T04:48:14.562678Z","shell.execute_reply.started":"2021-10-14T04:48:12.007397Z","shell.execute_reply":"2021-10-14T04:48:14.561677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Encode Y\n\ntag_tokenizer = Tokenizer()\n\ntag_tokenizer.fit_on_texts(Y)\n\nY_encoded = tag_tokenizer.texts_to_sequences(Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:17.092681Z","iopub.execute_input":"2021-10-14T04:48:17.092986Z","iopub.status.idle":"2021-10-14T04:48:18.761701Z","shell.execute_reply.started":"2021-10-14T04:48:17.092955Z","shell.execute_reply":"2021-10-14T04:48:18.760745Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Let's look at first encoded data point.\n\nprint(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\nprint('X: ', X[0], '\\n')\nprint('Y: ', Y[0], '\\n')\nprint()\nprint(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\nprint('X: ', X_encoded[0], '\\n')\nprint('Y: ', Y_encoded[0], '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:32.395310Z","iopub.execute_input":"2021-10-14T04:48:32.395773Z","iopub.status.idle":"2021-10-14T04:48:32.409974Z","shell.execute_reply.started":"2021-10-14T04:48:32.395708Z","shell.execute_reply":"2021-10-14T04:48:32.408871Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Make sure that each sequence of input and output is same length.\n\ndifferent_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n\nprint(\"{} sentences have different input-output lengths.\".format(sum(different_length)))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:40.862777Z","iopub.execute_input":"2021-10-14T04:48:40.863103Z","iopub.status.idle":"2021-10-14T04:48:40.889209Z","shell.execute_reply.started":"2021-10-14T04:48:40.863074Z","shell.execute_reply":"2021-10-14T04:48:40.888054Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Pad sequences","metadata":{}},{"cell_type":"markdown","source":"The next step after encoding the data is to **define the sequence lengths**. As of now, the sentences present in the data are of various lengths. We need to either pad short sentences or truncate long sentences to a fixed length. This fixed length, however, is a **hyperparameter**.","metadata":{}},{"cell_type":"code","source":"# Check length of longest sentence.\n\nlengths = [len(seq) for seq in X_encoded]\n\nprint(\"Length of longest sentence: {}\".format(max(lengths)))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:47.176136Z","iopub.execute_input":"2021-10-14T04:48:47.176734Z","iopub.status.idle":"2021-10-14T04:48:47.188749Z","shell.execute_reply.started":"2021-10-14T04:48:47.176688Z","shell.execute_reply":"2021-10-14T04:48:47.186768Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# View various lengths of sentences with the help of boxplot.\n\nsns.boxplot(lengths)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:48:50.270653Z","iopub.execute_input":"2021-10-14T04:48:50.270951Z","iopub.status.idle":"2021-10-14T04:48:50.521319Z","shell.execute_reply.started":"2021-10-14T04:48:50.270922Z","shell.execute_reply":"2021-10-14T04:48:50.520393Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"MAX_SEQ_LENGTH = 100  # Sequences greater than 100 in length will be truncated.\n\nX_padded = pad_sequences(X_encoded, maxlen = MAX_SEQ_LENGTH, padding = \"pre\", truncating = \"post\")\n\nY_padded = pad_sequences(Y_encoded, maxlen = MAX_SEQ_LENGTH, padding = \"pre\", truncating = \"post\")","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:49:26.227644Z","iopub.execute_input":"2021-10-14T04:49:26.228075Z","iopub.status.idle":"2021-10-14T04:49:27.624049Z","shell.execute_reply.started":"2021-10-14T04:49:26.228045Z","shell.execute_reply":"2021-10-14T04:49:27.623113Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Print the first sequence.\n\nprint(X_padded[0], \"\\n\"*3)\n\nprint(Y_padded[0])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-14T04:49:30.075117Z","iopub.execute_input":"2021-10-14T04:49:30.075410Z","iopub.status.idle":"2021-10-14T04:49:30.085192Z","shell.execute_reply.started":"2021-10-14T04:49:30.075380Z","shell.execute_reply":"2021-10-14T04:49:30.083821Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Assign padded sequences to X and Y.\n\nX, Y = X_padded, Y_padded","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:49:36.927399Z","iopub.execute_input":"2021-10-14T04:49:36.928078Z","iopub.status.idle":"2021-10-14T04:49:36.994647Z","shell.execute_reply.started":"2021-10-14T04:49:36.928046Z","shell.execute_reply":"2021-10-14T04:49:36.993683Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Word embeddings","metadata":{}},{"cell_type":"markdown","source":"Currently, each word and each tag is encoded as an integer. \n\nWe'll use a more sophisticated technique to represent the input words (X) using what's known as **word embeddings**.\n\nHowever, to represent each tag in Y, we'll simply use **one-hot encoding** scheme since there are only 13 tags in the dataset and the LSTM will have no problems in learning its own representation of these tags.","metadata":{}},{"cell_type":"markdown","source":"### Use word embeddings for input sequences (X)","metadata":{}},{"cell_type":"code","source":"# Using word2vec.\n\npath = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n\n# Load word2vec using the following function present in the gensim library.\n\nword2vec = KeyedVectors.load_word2vec_format(path, binary = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:49:54.320139Z","iopub.execute_input":"2021-10-14T04:49:54.320480Z","iopub.status.idle":"2021-10-14T04:50:17.879326Z","shell.execute_reply.started":"2021-10-14T04:49:54.320452Z","shell.execute_reply":"2021-10-14T04:50:17.878247Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Check word2vec effectiveness.\n\nword2vec.most_similar(positive = [\"King\", \"Woman\"], negative = [\"Man\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:50:20.926462Z","iopub.execute_input":"2021-10-14T04:50:20.927536Z","iopub.status.idle":"2021-10-14T04:50:20.977893Z","shell.execute_reply.started":"2021-10-14T04:50:20.927450Z","shell.execute_reply":"2021-10-14T04:50:20.976851Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Assign word vectors from word2vec model.\n\nEMBEDDING_SIZE  = 300\n\nVOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n\n# Create an empty embedding matix.\n\nembedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n\n# Create a word to index dictionary mapping.\n\nword2id = word_tokenizer.word_index\n\n# Copy vectors from word2vec model to the words present in corpus.\n\nfor word, index in word2id.items():\n    try:\n        embedding_weights[index, :] = word2vec[word]\n    except KeyError:\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:50:29.500808Z","iopub.execute_input":"2021-10-14T04:50:29.501097Z","iopub.status.idle":"2021-10-14T04:50:29.780605Z","shell.execute_reply.started":"2021-10-14T04:50:29.501067Z","shell.execute_reply":"2021-10-14T04:50:29.779559Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Check embedding dimension.\n\nprint(\"Embeddings shape: {}\".format(embedding_weights.shape))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:50:36.078418Z","iopub.execute_input":"2021-10-14T04:50:36.079234Z","iopub.status.idle":"2021-10-14T04:50:36.085511Z","shell.execute_reply.started":"2021-10-14T04:50:36.079200Z","shell.execute_reply":"2021-10-14T04:50:36.084044Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Let's look at an embedding of a word.\n\nembedding_weights[word_tokenizer.word_index['joy']]","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:50:38.624828Z","iopub.execute_input":"2021-10-14T04:50:38.625227Z","iopub.status.idle":"2021-10-14T04:50:38.643967Z","shell.execute_reply.started":"2021-10-14T04:50:38.625187Z","shell.execute_reply":"2021-10-14T04:50:38.642915Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Use one-hot encoding for output sequences (Y)","metadata":{}},{"cell_type":"code","source":"# Use Keras' to_categorical function to one-hot encode Y.\n\nY = to_categorical(Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:50:46.724642Z","iopub.execute_input":"2021-10-14T04:50:46.725516Z","iopub.status.idle":"2021-10-14T04:50:47.020615Z","shell.execute_reply.started":"2021-10-14T04:50:46.725468Z","shell.execute_reply":"2021-10-14T04:50:47.019762Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Print Y of the first output sequqnce.\n\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:50:50.924415Z","iopub.execute_input":"2021-10-14T04:50:50.925185Z","iopub.status.idle":"2021-10-14T04:50:50.932215Z","shell.execute_reply.started":"2021-10-14T04:50:50.925151Z","shell.execute_reply":"2021-10-14T04:50:50.930823Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Split data into training, validation and tesing sets","metadata":{}},{"cell_type":"code","source":"# Split entire data into training and testing sets.\n\nTEST_SIZE = 0.15\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = TEST_SIZE, random_state = 100)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:04.525074Z","iopub.execute_input":"2021-10-14T04:51:04.525393Z","iopub.status.idle":"2021-10-14T04:51:04.712692Z","shell.execute_reply.started":"2021-10-14T04:51:04.525362Z","shell.execute_reply":"2021-10-14T04:51:04.711602Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Split training data into training and validation sets.\n\nVALID_SIZE = 0.15\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = VALID_SIZE, random_state = 100)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:10.826774Z","iopub.execute_input":"2021-10-14T04:51:10.827246Z","iopub.status.idle":"2021-10-14T04:51:11.112979Z","shell.execute_reply.started":"2021-10-14T04:51:10.827191Z","shell.execute_reply":"2021-10-14T04:51:11.111630Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Print number of samples in each set.\n\nprint(\"TRAINING DATA\")\nprint('Shape of input sequences: {}'.format(X_train.shape))\nprint('Shape of output sequences: {}'.format(Y_train.shape))\nprint(\"-\"*50)\nprint(\"VALIDATION DATA\")\nprint('Shape of input sequences: {}'.format(X_validation.shape))\nprint('Shape of output sequences: {}'.format(Y_validation.shape))\nprint(\"-\"*50)\nprint(\"TESTING DATA\")\nprint('Shape of input sequences: {}'.format(X_test.shape))\nprint('Shape of output sequences: {}'.format(Y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:14.324804Z","iopub.execute_input":"2021-10-14T04:51:14.325201Z","iopub.status.idle":"2021-10-14T04:51:14.335814Z","shell.execute_reply.started":"2021-10-14T04:51:14.325155Z","shell.execute_reply":"2021-10-14T04:51:14.334584Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Before using RNN, we must make sure the dimensions of the data are what an RNN expects. In general, an RNN expects the following shape\n\nShape of X:\n(#samples, #timesteps, #features)\n\nShape of Y:\n(#samples, #timesteps, #features)\n\n![RNN tensor shape](./jupyter resources/rnn_tensor.png)\n\nNow, there can be various variations in the shape that you use to feed an RNN depending on the type of architecture. Since the problem we're working on has a many-to-many architecture, the input and the output both include number of timesteps which is nothing but the sequence length. But notice that the tensor X doesn't have the third dimension, that is, number of features. That's because we're going to use word embeddings before feeding in the data to an RNN, and hence there is no need to explicitly mention the third dimension. That's because when you use the Embedding() layer in Keras, the training data will automatically be converted to (#samples, #timesteps, #features) where #features will be the embedding dimention (and note that the Embedding layer is always the very first layer of an RNN). While using the embedding layer we only need to reshape the data to (#samples, #timesteps) which is what we have done. However, note that you'll need to shape it to (#samples, #timesteps, #features) in case you don't use the Embedding() layer in Keras.","metadata":{}},{"cell_type":"markdown","source":"### 2. Building Vanilla RNN model.","metadata":{}},{"cell_type":"markdown","source":"### Uninitialised fixed embeddings\nFirst let's try running a vanilla RNN. For this RNN we won't use the pre-trained word embeddings. We'll use randomly inititalise embeddings. Moreover, we won't update the embeddings weights.","metadata":{}},{"cell_type":"code","source":"# Total number of tags.\n\nNUM_CLASSES = Y.shape[2]","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:23.824404Z","iopub.execute_input":"2021-10-14T04:51:23.824947Z","iopub.status.idle":"2021-10-14T04:51:23.829824Z","shell.execute_reply.started":"2021-10-14T04:51:23.824899Z","shell.execute_reply":"2021-10-14T04:51:23.828060Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Create architecture.\n\nrnn_model = Sequential()\n\n# Create embedding layer.\nrnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         \n                        output_dim    =  EMBEDDING_SIZE,          \n                        input_length  =  MAX_SEQ_LENGTH,          \n                        trainable     =  False                    \n))\n\n# Add a RNN layer which contains 64 RNN cells.\n\nrnn_model.add(SimpleRNN(64, \n              return_sequences = True))\n\n# Add time distributed (output at each sequence) layer.\n                        \nrnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:31.256858Z","iopub.execute_input":"2021-10-14T04:51:31.257349Z","iopub.status.idle":"2021-10-14T04:51:34.357773Z","shell.execute_reply.started":"2021-10-14T04:51:31.257303Z","shell.execute_reply":"2021-10-14T04:51:34.356616Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Compile model","metadata":{}},{"cell_type":"code","source":"rnn_model.compile(loss      =  'categorical_crossentropy',\n                  optimizer =  'adam',\n                  metrics   =  ['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:43.188138Z","iopub.execute_input":"2021-10-14T04:51:43.188439Z","iopub.status.idle":"2021-10-14T04:51:43.211789Z","shell.execute_reply.started":"2021-10-14T04:51:43.188411Z","shell.execute_reply":"2021-10-14T04:51:43.210864Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Check summary of the model.\n\nrnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:51:51.561244Z","iopub.execute_input":"2021-10-14T04:51:51.561530Z","iopub.status.idle":"2021-10-14T04:51:51.572029Z","shell.execute_reply.started":"2021-10-14T04:51:51.561501Z","shell.execute_reply":"2021-10-14T04:51:51.570857Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Fit model","metadata":{}},{"cell_type":"code","source":"rnn_training = rnn_model.fit(X_train, Y_train, batch_size = 128, epochs = 10, validation_data = (X_validation, Y_validation))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:52:01.027163Z","iopub.execute_input":"2021-10-14T04:52:01.027453Z","iopub.status.idle":"2021-10-14T04:59:03.648524Z","shell.execute_reply.started":"2021-10-14T04:52:01.027424Z","shell.execute_reply":"2021-10-14T04:59:03.647651Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Visualise training history.\n\nplt.plot(rnn_training.history['acc'])\nplt.plot(rnn_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = \"lower right\")\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-14T04:59:14.072289Z","iopub.execute_input":"2021-10-14T04:59:14.072598Z","iopub.status.idle":"2021-10-14T04:59:14.311228Z","shell.execute_reply.started":"2021-10-14T04:59:14.072541Z","shell.execute_reply":"2021-10-14T04:59:14.310317Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Uninitialised trainable embeddings","metadata":{}},{"cell_type":"code","source":"# Create architecture.\n\nrnn_model = Sequential()\n\n# Create embedding layer.\n\nrnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         \n                        output_dim    =  EMBEDDING_SIZE,          \n                        input_length  =  MAX_SEQ_LENGTH,          \n                        trainable     =  True                     \n))\n\n# Add an RNN layer which contains 64 RNN cells.\n\nrnn_model.add(SimpleRNN(64, \n              return_sequences = True \n))\n\n# Add time distributed (output at each sequence) layer.\n\nrnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation = 'softmax')))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:59:22.891893Z","iopub.execute_input":"2021-10-14T04:59:22.892169Z","iopub.status.idle":"2021-10-14T04:59:23.180015Z","shell.execute_reply.started":"2021-10-14T04:59:22.892141Z","shell.execute_reply":"2021-10-14T04:59:23.179051Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Compile model","metadata":{}},{"cell_type":"code","source":"rnn_model.compile(loss      =  'categorical_crossentropy',\n                  optimizer =  'adam',\n                  metrics   =  ['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:59:29.167903Z","iopub.execute_input":"2021-10-14T04:59:29.168243Z","iopub.status.idle":"2021-10-14T04:59:29.185379Z","shell.execute_reply.started":"2021-10-14T04:59:29.168214Z","shell.execute_reply":"2021-10-14T04:59:29.184322Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Check summary of the model.\n\nrnn_model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-14T04:59:32.302468Z","iopub.execute_input":"2021-10-14T04:59:32.302798Z","iopub.status.idle":"2021-10-14T04:59:32.314424Z","shell.execute_reply.started":"2021-10-14T04:59:32.302768Z","shell.execute_reply":"2021-10-14T04:59:32.313401Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### Fit model","metadata":{}},{"cell_type":"code","source":"rnn_training = rnn_model.fit(X_train, Y_train, batch_size = 128, epochs = 10, validation_data = (X_validation, Y_validation))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T04:59:59.843016Z","iopub.execute_input":"2021-10-14T04:59:59.843636Z","iopub.status.idle":"2021-10-14T05:28:57.526755Z","shell.execute_reply.started":"2021-10-14T04:59:59.843592Z","shell.execute_reply":"2021-10-14T05:28:57.525944Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Visualise training history.\n\nplt.plot(rnn_training.history['acc'])\nplt.plot(rnn_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = \"lower right\")\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-14T05:29:03.852200Z","iopub.execute_input":"2021-10-14T05:29:03.852509Z","iopub.status.idle":"2021-10-14T05:29:04.090534Z","shell.execute_reply.started":"2021-10-14T05:29:03.852481Z","shell.execute_reply":"2021-10-14T05:29:04.089610Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### Using pre-trained embedding weights","metadata":{}},{"cell_type":"code","source":"# Create architecture.\n\nrnn_model = Sequential()\n\n# Create embedding layer.\n\nrnn_model.add(Embedding(input_dim     =  VOCABULARY_SIZE,         \n                        output_dim    =  EMBEDDING_SIZE,          \n                        input_length  =  MAX_SEQ_LENGTH,          \n                        weights       = [embedding_weights],     \n                        trainable     =  True                     \n))\n\n# Add an RNN layer which contains 64 RNN cells.\n\nrnn_model.add(SimpleRNN(64, \n              return_sequences = True  \n))\n\n# Add time distributed (output at each sequence) layer.\n\nrnn_model.add(TimeDistributed(Dense(NUM_CLASSES, activation = 'softmax')))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T05:29:38.422194Z","iopub.execute_input":"2021-10-14T05:29:38.422492Z","iopub.status.idle":"2021-10-14T05:29:38.805354Z","shell.execute_reply.started":"2021-10-14T05:29:38.422463Z","shell.execute_reply":"2021-10-14T05:29:38.804277Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"### Compile model","metadata":{}},{"cell_type":"code","source":"rnn_model.compile(loss      =  'categorical_crossentropy',\n                  optimizer =  'adam',\n                  metrics   =  ['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T05:29:47.142198Z","iopub.execute_input":"2021-10-14T05:29:47.142498Z","iopub.status.idle":"2021-10-14T05:29:47.157670Z","shell.execute_reply.started":"2021-10-14T05:29:47.142470Z","shell.execute_reply":"2021-10-14T05:29:47.156521Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Check summary of the model.\n\nrnn_model.summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-14T05:29:49.696303Z","iopub.execute_input":"2021-10-14T05:29:49.696662Z","iopub.status.idle":"2021-10-14T05:29:49.707539Z","shell.execute_reply.started":"2021-10-14T05:29:49.696632Z","shell.execute_reply":"2021-10-14T05:29:49.706490Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Fit model","metadata":{}},{"cell_type":"code","source":"rnn_training = rnn_model.fit(X_train, Y_train, batch_size = 128, epochs = 10, validation_data = (X_validation, Y_validation))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T05:29:58.609408Z","iopub.execute_input":"2021-10-14T05:29:58.609937Z","iopub.status.idle":"2021-10-14T05:59:34.233658Z","shell.execute_reply.started":"2021-10-14T05:29:58.609901Z","shell.execute_reply":"2021-10-14T05:59:34.232766Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Visualise training history.\n\nplt.plot(rnn_training.history['acc'])\nplt.plot(rnn_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = \"lower right\")\nplt.show()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-14T05:59:38.954662Z","iopub.execute_input":"2021-10-14T05:59:38.954980Z","iopub.status.idle":"2021-10-14T05:59:39.563218Z","shell.execute_reply.started":"2021-10-14T05:59:38.954950Z","shell.execute_reply":"2021-10-14T05:59:39.562311Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### 2. Building LSTM model","metadata":{}},{"cell_type":"markdown","source":"We'll use pre-trained word embeddings in following models and allow them to be updated as well. ","metadata":{}},{"cell_type":"markdown","source":"### Create model architecture","metadata":{}},{"cell_type":"code","source":"# Create architecture.\n\nlstm_model = Sequential()\n\nlstm_model.add(Embedding(input_dim     = VOCABULARY_SIZE,         \n                         output_dim    = EMBEDDING_SIZE,         \n                         input_length  = MAX_SEQ_LENGTH,          \n                         weights       = [embedding_weights],     \n                         trainable     = True                      \n))\n\nlstm_model.add(LSTM(64, return_sequences = True))\n\nlstm_model.add(TimeDistributed(Dense(NUM_CLASSES, activation = 'softmax')))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T05:59:48.161842Z","iopub.execute_input":"2021-10-14T05:59:48.162600Z","iopub.status.idle":"2021-10-14T05:59:48.748959Z","shell.execute_reply.started":"2021-10-14T05:59:48.162541Z","shell.execute_reply":"2021-10-14T05:59:48.748022Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Compile model","metadata":{}},{"cell_type":"code","source":"lstm_model.compile(loss      =  'categorical_crossentropy',\n                   optimizer =  'adam',\n                   metrics   =  ['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T05:59:54.981643Z","iopub.execute_input":"2021-10-14T05:59:54.981955Z","iopub.status.idle":"2021-10-14T05:59:54.998347Z","shell.execute_reply.started":"2021-10-14T05:59:54.981923Z","shell.execute_reply":"2021-10-14T05:59:54.996597Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Check summary of the model.\n\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T05:59:57.282004Z","iopub.execute_input":"2021-10-14T05:59:57.282320Z","iopub.status.idle":"2021-10-14T05:59:57.295903Z","shell.execute_reply.started":"2021-10-14T05:59:57.282292Z","shell.execute_reply":"2021-10-14T05:59:57.294880Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Fit model","metadata":{}},{"cell_type":"code","source":"lstm_training = lstm_model.fit(X_train, Y_train, batch_size = 128, epochs = 10, validation_data = (X_validation, Y_validation))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:00:12.656875Z","iopub.execute_input":"2021-10-14T06:00:12.657240Z","iopub.status.idle":"2021-10-14T06:23:32.697453Z","shell.execute_reply.started":"2021-10-14T06:00:12.657207Z","shell.execute_reply":"2021-10-14T06:23:32.696544Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Visualise training history.\n\nplt.plot(lstm_training.history['acc'])\nplt.plot(lstm_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = \"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:23:37.411367Z","iopub.execute_input":"2021-10-14T06:23:37.411695Z","iopub.status.idle":"2021-10-14T06:23:37.666244Z","shell.execute_reply.started":"2021-10-14T06:23:37.411662Z","shell.execute_reply":"2021-10-14T06:23:37.665396Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### 2. Building GRU model","metadata":{}},{"cell_type":"markdown","source":"### Create model architecture","metadata":{}},{"cell_type":"code","source":"# Create architecture.\n\ngru_model = Sequential()\n\ngru_model.add(Embedding(input_dim     = VOCABULARY_SIZE,\n                        output_dim    = EMBEDDING_SIZE,\n                        input_length  = MAX_SEQ_LENGTH,\n                        weights       = [embedding_weights],\n                        trainable     = True\n))\n\ngru_model.add(GRU(64, return_sequences = True))\n\ngru_model.add(TimeDistributed(Dense(NUM_CLASSES, activation = 'softmax')))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:23:46.115237Z","iopub.execute_input":"2021-10-14T06:23:46.115628Z","iopub.status.idle":"2021-10-14T06:23:46.770928Z","shell.execute_reply.started":"2021-10-14T06:23:46.115549Z","shell.execute_reply":"2021-10-14T06:23:46.769966Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Compile model","metadata":{}},{"cell_type":"code","source":"gru_model.compile(loss = 'categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:23:55.323146Z","iopub.execute_input":"2021-10-14T06:23:55.323769Z","iopub.status.idle":"2021-10-14T06:23:55.339124Z","shell.execute_reply.started":"2021-10-14T06:23:55.323715Z","shell.execute_reply":"2021-10-14T06:23:55.337800Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Check summary of model.\n\ngru_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:23:57.641479Z","iopub.execute_input":"2021-10-14T06:23:57.641920Z","iopub.status.idle":"2021-10-14T06:23:57.656206Z","shell.execute_reply.started":"2021-10-14T06:23:57.641856Z","shell.execute_reply":"2021-10-14T06:23:57.655084Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### Fit model","metadata":{}},{"cell_type":"code","source":"gru_training = gru_model.fit(X_train, Y_train, batch_size = 128, epochs = 10, validation_data = (X_validation, Y_validation))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:24:25.314882Z","iopub.execute_input":"2021-10-14T06:24:25.315177Z","iopub.status.idle":"2021-10-14T06:47:24.081746Z","shell.execute_reply.started":"2021-10-14T06:24:25.315148Z","shell.execute_reply":"2021-10-14T06:47:24.080863Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Visualise training history.\n\nplt.plot(gru_training.history['acc'])\nplt.plot(gru_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = \"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:47:28.496169Z","iopub.execute_input":"2021-10-14T06:47:28.496512Z","iopub.status.idle":"2021-10-14T06:47:28.724819Z","shell.execute_reply.started":"2021-10-14T06:47:28.496483Z","shell.execute_reply":"2021-10-14T06:47:28.723866Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"### 3. Building Bidirectional LSTM model","metadata":{}},{"cell_type":"markdown","source":"### Create model architecture","metadata":{}},{"cell_type":"code","source":"# Create architecture.\n\nbidirect_model = Sequential()\n\nbidirect_model.add(Embedding(input_dim     = VOCABULARY_SIZE,\n                             output_dim    = EMBEDDING_SIZE,\n                             input_length  = MAX_SEQ_LENGTH,\n                             weights       = [embedding_weights],\n                             trainable     = True\n))\n\nbidirect_model.add(Bidirectional(LSTM(64, return_sequences = True)))\n\nbidirect_model.add(TimeDistributed(Dense(NUM_CLASSES, activation = 'softmax')))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:47:42.101474Z","iopub.execute_input":"2021-10-14T06:47:42.101833Z","iopub.status.idle":"2021-10-14T06:47:42.973161Z","shell.execute_reply.started":"2021-10-14T06:47:42.101775Z","shell.execute_reply":"2021-10-14T06:47:42.972009Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"### Compile model","metadata":{}},{"cell_type":"code","source":"bidirect_model.compile(loss = 'categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:47:48.175643Z","iopub.execute_input":"2021-10-14T06:47:48.175974Z","iopub.status.idle":"2021-10-14T06:47:48.192801Z","shell.execute_reply.started":"2021-10-14T06:47:48.175941Z","shell.execute_reply":"2021-10-14T06:47:48.191645Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Check summary of model.\n\nbidirect_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:47:52.611205Z","iopub.execute_input":"2021-10-14T06:47:52.611528Z","iopub.status.idle":"2021-10-14T06:47:52.627001Z","shell.execute_reply.started":"2021-10-14T06:47:52.611499Z","shell.execute_reply":"2021-10-14T06:47:52.625788Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"### Fit model","metadata":{}},{"cell_type":"code","source":"bidirect_training = bidirect_model.fit(X_train, Y_train, batch_size = 128, epochs = 10, validation_data = (X_validation, Y_validation))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T06:48:09.362396Z","iopub.execute_input":"2021-10-14T06:48:09.362699Z","iopub.status.idle":"2021-10-14T07:11:37.011712Z","shell.execute_reply.started":"2021-10-14T06:48:09.362668Z","shell.execute_reply":"2021-10-14T07:11:37.010548Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Visualise training history.\n\nplt.plot(bidirect_training.history['acc'])\nplt.plot(bidirect_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = \"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:11:40.451333Z","iopub.execute_input":"2021-10-14T07:11:40.451892Z","iopub.status.idle":"2021-10-14T07:11:40.692443Z","shell.execute_reply.started":"2021-10-14T07:11:40.451859Z","shell.execute_reply":"2021-10-14T07:11:40.691492Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### 5. Model evaluation","metadata":{}},{"cell_type":"code","source":"loss, accuracy = rnn_model.evaluate(X_test, Y_test, verbose = 1)\n\nprint(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:11:53.111635Z","iopub.execute_input":"2021-10-14T07:11:53.112231Z","iopub.status.idle":"2021-10-14T07:11:56.962536Z","shell.execute_reply.started":"2021-10-14T07:11:53.112201Z","shell.execute_reply":"2021-10-14T07:11:56.961631Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = lstm_model.evaluate(X_test, Y_test, verbose = 1)\n\nprint(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:12:04.174532Z","iopub.execute_input":"2021-10-14T07:12:04.175417Z","iopub.status.idle":"2021-10-14T07:12:05.970455Z","shell.execute_reply.started":"2021-10-14T07:12:04.175384Z","shell.execute_reply":"2021-10-14T07:12:05.969470Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = gru_model.evaluate(X_test, Y_test, verbose = 1)\n\nprint(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:12:15.571706Z","iopub.execute_input":"2021-10-14T07:12:15.572032Z","iopub.status.idle":"2021-10-14T07:12:17.300070Z","shell.execute_reply.started":"2021-10-14T07:12:15.572001Z","shell.execute_reply":"2021-10-14T07:12:17.299051Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = bidirect_model.evaluate(X_test, Y_test, verbose = 1)\n\nprint(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T07:12:26.363904Z","iopub.execute_input":"2021-10-14T07:12:26.364265Z","iopub.status.idle":"2021-10-14T07:12:28.768998Z","shell.execute_reply.started":"2021-10-14T07:12:26.364221Z","shell.execute_reply":"2021-10-14T07:12:28.768067Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"### Conclusions:\n\nAccuracies of different RNN models are as follows:\n\n1. Vanilla RNN model - 98.99%\n2. LSTM model - 99.11%\n3. GRU model - 99.09%\n4. Bidirectional LSTM model - 99.32%\n\nBidirectional LSTM model is performing best with an accuracy of 99.32% for POS tagging when compared to other RNN models.","metadata":{}}]}